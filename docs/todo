Singh:
  Try replicating dt_rewards results with fixed gamma, hopting lr
  Benchmark feature rep. assumption: include_self or not in neighs4
  Why is performance worse than 12.1% w/o hoffs?
  Results seem to get worse over time. Try long run. How to fix?
          - Lagging net
          - LR decay

QNet:
  Duelling qnet pre-training
  Why is loss increasing for long runs?
  Pre-training: Shuffled vs sequential
  Can the afterstate approach be combined with duelling qnet?
  Block prob first 15k iters are EXACTLY the same with/without qnet freps
  Try simple exp replay (Juliani) too see if it performs the same
      (to check if current impl has bugs)
  Prioritized replay
  Parameter noise instead of epsilon-greedy
  multiple linear layers with Relu
  wolf learning rate

QNet + Singh hybrid:
  Train on features alone
  Including convs. for Singh (1x1 before dense?)



Progging:
  Retrieve pp from mongodb
  Batch norm for exp replay

Reading:
  Look up neural net output repr for Go
       - is it better to have 7x7x70 outputs, one for each cell-channel pair?
