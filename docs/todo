Singh:
  Try replicating dt_rewards results with fixed gamma, hopting lr
  Benchmark feature rep. assumption: include_self or not in neighs4
  Why is performance worse than 12.1% w/o hoffs?
  Results seem to get worse over time. Try long run. How to fix?
          - Lagging net
          - LR decay

QNet:
  Test argmaxing in opt_ch on end-events
  Duelling qnet pre-training
  Why is loss increasing for long runs?
  Pre-training: Shuffled vs sequential
  Can the afterstate approach be combined with duelling qnet?

QNet + Singh hybrid:
     Including features for Qnet
     Including convs. for Singh (1x1 before dense?)


Progging:
  Store pp in mongodb
  Batch norm for exp replay

Reading:
  Look up neural net output repr for Go
       - is it better to have 7x7x70 outputs, one for each cell-channel pair?
