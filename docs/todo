Singh:
  Try replicating dt_rewards results with fixed gamma, hopting lr
  Benchmark feature rep. assumption: include_self or not in neighs4
  Why is performance worse than 12.1% w/o hoffs?
  Results seem to get worse over time. Try long run. How to fix?
          - Lagging net
          - LR decay
  Including convs. for Singh (1x1 before dense?)


QNet:
  Boltzmann explore.
  Exponential decay of epsilon seems excessive. Should start lower, decay slower.
  Try dt rewards
  Duelling qnet pre-training
  Why is loss increasing for long runs?
  Pre-training: Shuffled vs sequential
  Can the afterstate approach be combined with duelling qnet?
  Block prob first 15k iters are EXACTLY the same with/without qnet freps
  Try simple exp replay (Juliani) too see if it performs the same
      (to check if current impl has bugs)
  Prioritized replay
  Parameter noise instead of epsilon-greedy
  multiple linear layers with Relu
  wolf learning rate. is it applicable? how did morosz use it
  experiment qnet arch
  RNN
  Train on features alone
  is it better to have 7x7x70 outputs, one for each cell-channel pair?



Progging:
  Retrieve pp from mongodb
  Batch norm for exp replay
