Distributed system with multiple agents. Cooperation or competition?
Sharing of parameters?

Thesis notes:
For the memory-less traffic modelled in the simulator, a RL-compatible model
can be derived by using the fact that a) the minimum of several exponentially
distributed is itself exp. dist. with some parameter, and b) given an action,
the next allocation map is deterministic and hence the reward is deterministic.
However, as previously discussed, this model might not be representable for real-world
traffic and so this approach could only be used in simulation and 
not in a deployed system.

maybe dont want to do decision time planning or rollouts, e.g mtcs, in deployed sys
because of response time

for lagging qnet, specify which net is used to selection actions which are executed
and why this net and not the other is used

guide on medium.com:
"Instead of updating the target network periodically and 
all at once, we will be updating it frequently, but slowly. 
This technique was introduced in another DeepMind paper 1509.02971.pdf
earlier this year, where they found that it stabilized the training process.

double qnet
http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847

explicitly note the terminology difference between free and eligible channels

could/should have a section in front in addition to abbreviations, covering 
notation (expectations), variables(q vs Q, gamma, beta, alpha ...). See deep rl book.

mention that beta affects the absolute scale of the gradient, hence also interplays
with the learning rate

Singh: There are better options than SGD for linear nets 
(which? Least squares TD, Newtons method?). Thesis subject is deep RL, so it's irrelevant.

Src Bertsekas book:
Semi-Markov: Controls applied at discrete times, but rewards are continuously accumulated.
Time between successive control choices vary, therefore so do time between state transitions.

https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565
The other problem is that the “true” reward structure is always a moving target.
As an agent improves its policy, then the expected rewards should increase as
well. This causes the unintuitive experience of the loss function increasing,
even though the agent’s policy is more successful.

-----

dt-rewards/beta performs better than gamma (at least on singh). Why?
The actions of the agent does not affect the duration of calls.
Three explanations:
  a) A scaled reward (much lower in value) is easier to learn, ref Deep RL that Matters
  b) For a given Q, reward and next Q, the loss will vary according to the time between
     events (will it??). So in effect it acts as learning rate noise. noisy learning rates
     may improve network convergence, ref paper with cyclic /triangular LR
  c) For a given Q, reward and next Q, the weight of reward vs next Q will vary, thus
     so emulates noisy gamma.

----

soft start training:
gradually increase call rate, to prevent the system to being forced into an awkward corner
before it has learned.

-------
could should sate act state space explicitly for dca, see eg usaha
